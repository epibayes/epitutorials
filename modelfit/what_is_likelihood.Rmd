---
title: "Likelihood and Model Fit: A Visual Tour"
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
description: "Likelihood and Model Fit: A Visual Tour"
---
---

```{r setup, include=FALSE}
require(learnr)
require(ggplot2)
require(plotly)
require(lhs)
require(dplyr)
require(lme4)
knitr::opts_chunk$set(echo = FALSE)
```

## What is Likelihood?
### A Brief Definition and its Usage
Likelihood is a concept that underlies most statistical modeling that falls under the heading of [generalized linear models](https://en.wikipedia.org/wiki/Generalized_linear_model) or GLMs.

When we fit any kind of statistical model to a dataset, the goal is to find solutions that either **maximize the likelihood** of the data given the model (under a frequentist, maximum likelihood estimation framework), or maximize the likelihood of the data given the data and some **prior information on the value of the parameters** (under a more Bayesian framework).

## Simulating Some Data
### A Normal Distribution
To get a better feel for what this looks like, let's first generate some data from a normal distribution. Play around with the sliders on the left side and see what comes out on the right side.

```{r, echo=FALSE}
sidebarPanel(
sliderInput("mu", "Mean:", min = 0.1, max = 5, value = 2),
sliderInput("norm_sd", "Standard Deviation:", min = 0.1, max = 4, value = 1)
)

mainPanel(
plotlyOutput("normHist"),
)
```

```{r, context="server"}
norm_vals <- reactive({rnorm(100, input$mu, input$norm_sd)})

norm_pos <- reactive({rnorm(length(norm_vals()))
                            })
  
  norm_ll <- reactive({dnorm(norm_vals(), input$mu_guess, input$sd_guess, log=TRUE)})

norm_slice <- reactive({purrr::map_dbl(seq(from=0.1, to=10, by=0.1), function(x) 
  {
  sum(dnorm(norm_vals(), input$mu_guess,x,log=TRUE))
  })
})
  
output$normHist <- renderPlotly({
  g <- ggplot(data.frame(x=norm_vals())) + geom_dotplot(aes(x=x),dotsize=0.4) + xlab("Value") + ylab("Density")
  return(ggplotly(g))


})


  output$normLL <- renderPlotly({
    df <- data.frame(x=norm_vals())
    df$y <- norm_pos()
    g2 <- ggplot(data=df) + geom_point(aes(x=x,y=y,colour=norm_ll()), size=2) + theme(axis.title.y=element_blank(),
                                                        axis.text.y=element_blank(),
                                                        axis.ticks.y=element_blank()) +
      
      geom_vline(xintercept=input$mu_guess) +
      geom_segment(aes(x=input$mu_guess-input$sd_guess, xend=input$mu_guess+input$sd_guess,y=0,yend=0),arrow = arrow()) + 
      scale_colour_gradient(name="Log-likelihood",limits=c(-10,0)) 
    
    return(ggplotly(g2))  
  })
  
  output$normSlice <- renderPlotly({
    df <- data.frame(x= seq(from=0.1, to=10, by=0.1), y=norm_slice())
    
   g <- ggplot(data=df) + geom_point(aes(x=x,y=y), size=2) + coord_cartesian(ylim=c(-500,0)) +
     geom_vline(xintercept=input$sd_guess) + ylab("Total log-likelihood") + xlab("SD Guess")
   
   return(ggplotly(g))
  })

```

### Bringing in the Likelihood

Now let's take a look at our data in terms of the **likelihood** they were generated by a model we are trying to fit to the data.

We're going to pretend that we don't know anything about our data except that they are approximately normally distributed. So, the goal of our modeling exercise is just to find the mean and standard deviation that best explain the data.

In this case, we're just going to manually try to find values of the **mean** and **standard deviation** of our simulated data that do a reasonable job of representing the data. Normally some kind of algorithm does the work for us, but we're doing it this way to get a better feel for how this works.

In the plot below, the x positions represent the value of one simulated data point from the histogram above, and the colors of the points represent their log-likelihood: the **brighter** the point, the more likely it was generated by our model. We use the logarithm of the likelihood instead of its raw value because the raw value can be quite small and because small parameter value changes can cause large changes in the likelihood. Working with the log often makes it easier to find the maximum value.

In this example, the vertical line shows you the value of your guess for the mean, and the width of the horizontal line shows you the standard deviation of the data:

```{r, echo=FALSE}
sidebarPanel(
sliderInput("mu_guess", "Mean:", min = -5, max = 5, value = 2)
)

mainPanel(
plotlyOutput("normLL")

)
```

### Fit the Standard Deviation

Now, instead of fitting just the mean, we can also try to find a value of the standard deviation that maximizes the likelihood of the data. Each point on the plot below shows you the total log-likelihood of all the points you sampled from that normal distribution above.

The vertical line is fixed at the standard deviation of the data, and you can see that it's near the maximum, i.e. where the data are most likely to be generated by our simple model.

Try moving the guess for the standard deviation around and see how it changes the color of the individual points above as well as the overall likelihood of the data.

```{r, echo=FALSE}

sidebarPanel(
  sliderInput("sd_guess", "Standard Deviation:", min = 0.1, max = 10, value = 1)
)
mainPanel(
  plotlyOutput("normSlice")
)
```

```{r,context="server"}
observe({
  val <- input$norm_sd
  updateSliderInput(session, "sd_guess", value=val)
})

```

Now go back up and move the mean around and see how it changes the shape of the likelihood surface for the dataset.

Which one of these has more of an impact on the likelihood, **mean** or **standard deviation**?

## Using a Linear Model
### A Simple Linear Model

Now our task is to take what we just learned and use it to put together a linear model of the classic sort, i.e. $y_i \sim Normal(\mu_i, \sigma)$.

This only differs from the previous example in the sense that we have added an additional variable along the x-axis, but otherwise the ideas are very similar.

```{r, echo=FALSE}
sidebarPanel(
sliderInput("beta", "Slope:", min = 0.1, max = 5, value = 2),
sliderInput("alpha", "Intercept:", min = 0.1, max = 5, value = 0.1),

sliderInput("sd", "Error Standard Deviation:", min = 0.1, max = 4, value = 1)
)

mainPanel(
plotlyOutput("distPlot")
)
```


```{r, context="server"}
xvals <- seq(from = 0.0, to = 10, by  = 0.05)
y <- reactive({rnorm(length(xvals), input$alpha + input$beta*xvals, input$sd)})
real_ll <- reactive({dnorm(y(), input$alpha + input$beta*xvals, input$sd, log=TRUE)})

output$distPlot <- renderPlotly({
  g <- ggplot(data.frame(x=xvals, y=y()), aes(x=x)) + 
    geom_point(aes(y=y), size=2) + 
    xlim(0, 10) + 
    ylim(0, 60) 
  return(ggplotly(g))
})

```

### Try to Find the Maximum Likelihood

The line in the figure below is a regression line with intercept and slope set by the sliders. 

Move the sliders around and see if you can find where the model maximizes the **likelihood** of the data (darker colors = lower likelihood/worse fit):

```{r, echo = FALSE}

sidebarPanel(
sliderInput("beta_pred", "Slope:", min = -5, max = 5, value = 0),
sliderInput("alpha_pred", "Intercept:", min = -10, max = 10, value = 0),

sliderInput("sd_pred", "Error Standard Deviation:", min = 0.1, max = 4, value = 1)
)

mainPanel(
plotlyOutput("llPlot")
)

```

```{r, context="server"}
pred_y <-  reactive({input$alpha_pred + input$beta_pred*xvals})

data_logll <- reactive({
  z <- dnorm(y(), pred_y(), input$sd_pred, log = TRUE) 
  return(z-real_ll())
          })


output$llPlot <- renderPlotly({
  g <- ggplot(data.frame(x=xvals, y=y(), yp = pred_y(), z=data_logll()), aes(x=x)) + 
    geom_point(aes(y=y,color=z), size=2) + 
    geom_line(aes(y=yp)) +
    xlim(0, 10) + 
    ylim(-60, 60) +
    guides(color=guide_legend(title="Difference from MLE")) + 
    scale_color_distiller(name = "Difference from MLE", type="div", palette="spectral",limits=c(-5,0))

  ggplotly(g)
})

```

What impact does the variance seem to have relative to the intercept and slope?


## Adding an **Exposure**

Up to this point, we have focused on the relationship between a single variable and an outcome. Now let's make things a bit more interesting and look at what happens when our data have very simple **clustering**.

### Observing the Impact of Clustering
Here we are assuming that **increased** household wealth is associated with decreased systolic blood pressure (SBP) as in the Merlo example. In our version, though, we're imagining there are just two neighborhoods: A 'normal' one where the relationship between wealth and SBP holds, and an 'outlier' one that is associated with an **increase** in SBP for anyone who lives there, regardless of wealth.

The points in the figure represent SBP observations for individuals in each neighborhood, and the regression line shows the predictions of a model that ignores this clustering and just estimates the intercept and wealth effects.

```{r, echo=FALSE}
sidebarLayout(
sidebarPanel(
sliderInput("beta_1", "Wealth Effect:", min = -10, max = 0, value = 0),
sliderInput("beta_2", "Outlier Neighborhood Effect:", min = 0, max = 50, value = 10),
sliderInput("alpha_bp", "Intercept:", min = 120, max = 200, value = 150),
),

mainPanel(
  plotOutput("clusterPlot"),
  
)
)
```


```{r, context="server"}
xvals <- seq(from = 0.0, to = 10, by  = 0.05)

y_cluster <- reactive({
  
  ## Sample proportion in outlier area
  p_outlier <- plogis(qlogis(input$p_n) + log(input$o_n)*xvals)
  outlier_n <- rbinom(length(xvals), 1, p_outlier)
  
  ## Sample outcome values
  y_hat <- input$alpha_bp + input$beta_1*xvals + input$beta_2*outlier_n
  vals <- rnorm(length(xvals), y_hat, input$sd)
  true_ll <- dnorm(vals, y_hat, input$sd, log=TRUE)
  
  return(data.frame(x = xvals,
                    y = vals,
                    outlier = outlier_n,
                    logll= true_ll))
        })

  model_fit <- reactive({
    if (input$adjust == TRUE) {
      fit <- lm(y ~ x + outlier, data = y_cluster())
      names(fit$coefficients) <- c("Intercept", "Wealth", "Outlier")
      
    } else {

     fit <- lm(y ~ x, data = y_cluster())
       names(fit$coefficients) <- c("Intercept", "Wealth")
    }
    return(fit)
  })
  
  output$model_summary <- renderPrint({
    summary(model_fit())
  })


output$clusterPlot <- renderPlot({
  
  point_ll <- dnorm(y_cluster()$y, predict(model_fit()), input$sd, log=TRUE) - y_cluster()$logll
  
  g <- ggplot(y_cluster(), aes(x=x)) + 
    geom_point(aes(y=y, colour = point_ll), size=5) + 
    xlim(0, 10) + 
    ylim(0, 210)
  
  if (input$adjust == "TRUE") {
      g <- g + geom_smooth(method = "lm", aes(y=y, group=outlier))
  } else {
      g <- g + geom_smooth(method = "lm", aes(y=y))

  }
  plot(g)
})

```

```{r, echo=FALSE}
fluidRow(
  column(
    3,
    sliderInput(
      "p_n",
      "Probability that Poorest (Wealth = 0) Live in Outlier Neighborhood:",
      min = 0.01,
      max = 0.5,
      value = 0.5
    ),
    
    sliderInput(
      "o_n",
      "Odds Ratio for Residence in Outlier for 1-Point Increase in Wealth:",
      min = 0.5,
      max = 1.0,
      value = 1
    ),
    
    checkboxInput("adjust", "Adjust for Neighborhood:", FALSE)
  ),
  column(9,
         verbatimTextOutput("model_summary"))
)

```

Above, you can see the results of fitting a linear model to the data. Take a look at the estimated **intercept** and **slope** for the wealth effect.

#### Some Questions

1. What happens to these estimates when you make residence in the outlier neighborhood **dependent** on wealth by making the odds of living in the outlier neighborhood **decrease** with wealth?
 
2. What happens when you adjust for neighborhood? How is the effect of that impacted by the parameters for who is selected into each neighborhood?

<!-- ## More clustering! -->

<!-- Now, let's go crazy with a more multilevel example. We'll repeat the example from the last page, but imagine individuals are grouped into discrete neighborhoods by wealth (not an unrealistic assumption), and that wealth remains protective against SBP. -->

<!-- Now, we'll let some factor that is potentially associated with SBP risk, e.g. walkability, vary across neighborhoods, also as a function of *increasing neighborhood-level wealth*.  -->

<!-- Play with the sliders below, and see what happens to the across- and within-neighborhood relationships between wealth and SBP as you modify the intensity of the neighborhood risk factor. -->

<!-- ```{r} -->
<!-- sidebarPanel( -->

<!-- sliderInput("beta_1_n", "Wealth Effect:", min = -10, max = 0, value = -5), -->
<!-- sliderInput("beta_2_n", "Neighborhood Effect:", min = 0, max = 50, value = 0), -->
<!-- sliderInput("alpha_n", "Intercept:", min = 120, max = 200, value = 150), -->

<!-- ) -->

<!-- mainPanel( -->
<!--  plotOutput("neighborhoodPlot") -->
<!-- ) -->


<!-- ``` -->

<!-- Now, let's see what this looks like in modeling terms when we do and don't adjust for neighborhood: -->

<!-- ```{r} -->
<!-- sidebarPanel( -->

<!-- checkboxInput("n_adjust", "Adjust for neighborhood:", FALSE) -->

<!-- ) -->

<!-- mainPanel( -->
<!--   verbatimTextOutput("n_model_summary") -->
<!-- ) -->

<!-- ``` -->



<!-- Now, try to see if you can completely wash out the impact of wealth vs. neighborhood. -->

<!-- After that, see if you can get the wealth effect to *reverse*! -->

<!-- ## Space!  -->

<!-- ### The Final Frontier? -->
<!-- ```{r, fig.cap=""} -->
<!-- knitr::include_graphics("images/star_trek_tng.jpg") -->
<!-- ``` -->

<!-- Now, we will revisit the neighborhood effects example from the last section, but will take a look at these neighborhoods in geographic space. -->

<!-- ```{r} -->
<!-- sidebarLayout( -->
<!-- sidebarPanel( -->
<!-- h4("Manipulate the relationship between activity and SBP:"), -->

<!-- sliderInput("beta_s", "Activity Effect:", min = -30, max = 0, value = -5), -->
<!-- sliderInput("alpha_s", "Intercept:", min = 120, max = 200, value = 150), -->

<!-- h4("Manipulate the spatial correlation in activity level:"), -->
<!-- sliderInput("rho_s", "Spatial Correlation:", min = 0, max = 0.99, value = 0) -->


<!-- ), -->

<!-- mainPanel( -->
<!-- tabsetPanel(type = "tabs", -->
<!--  tabPanel("Individuals", plotOutput("pointSpatialPlot")), -->
<!--  tabPanel("Neighborhood", plotOutput("neighborhoodSpatialPlot")) -->
<!-- ) -->
<!-- ) -->
<!-- ) -->
<!-- ``` -->



<!-- ### What's left? -->

<!-- Now, let's see what this looks like in modeling terms when we do and don't adjust for physical acticity: -->

<!-- ```{r} -->
<!-- sidebarLayout( -->
<!-- sidebarPanel( -->

<!-- checkboxInput("s_adjust", "Adjust for activity:", FALSE) -->

<!-- ), -->

<!-- mainPanel( -->
<!--   tabsetPanel(type="tabs", -->
<!--    tabPanel("Individual Residual", plotOutput("pointSpatialResidPlot")), -->
<!--  tabPanel("Neighborhood Residual", plotOutput("neighborhoodSpatialResidPlot")), -->
<!--    tabPanel("Model", verbatimTextOutput("s_model_summary")) -->
<!--  ) -->
<!-- )) -->

<!-- ``` -->

<!-- #### Questions -->

<!-- 1. What happens to the residual as you increase the intensity of spatial autocorrelation? -->

<!-- 2. Does this appear to have a bigger effect on the neighborhood or point-level residuals? -->


<!-- ```{r, context = "server"} -->

<!-- # generate autocorrelated data. -->
<!-- autocor_outcome <- function(n,a,b,corr) { -->

<!-- ## Randomly scatter points on a 3 x 3 grid representing -->
<!-- ## 9 neighborhoods -->
<!-- x <- runif(n,0,3) -->
<!-- y <- runif(n,0,3) -->

<!-- cases <- data.frame(id = 1:n,  -->
<!--                     x = x,  -->
<!--                     y = y) -->

<!-- case_dist <- as.matrix(dist(cbind(cases$x,cases$y))) -->

<!-- ## Set up the neighborhood indices -->
<!-- neighborhood_loc <- expand.grid(1:3, 1:3) -->
<!-- names(neighborhood_loc) <- c("nx","ny") -->
<!-- neighborhood_loc$neighborhood <- 1:nrow(neighborhood_loc) -->
<!-- neighborhood_loc$walkability <- sample(1:9) -->

<!-- cases <- cases %>% -->
<!--   mutate(nx = ceiling(x), -->
<!--          ny = ceiling(y)) %>% -->
<!--   inner_join(neighborhood_loc) -->

<!-- # fake, uncorrelated observations -->
<!-- X = rnorm(nrow(cases)) -->

<!-- ############################################### -->
<!-- # fake sigma... correlated decreases distance. -->
<!-- sigma = diag(nrow(cases)) -->
<!-- sigma <- corr ^ case_dist -->

<!-- ############################################### -->

<!-- # Y is autocorrelated... -->
<!-- Y <- t(X %*% chol(sigma)) -->
<!-- cases$activity <- Y - mean(Y) -->

<!-- cases$z <- a + b*cases$activity + rnorm(nrow(cases),0,1) -->

<!-- cases$neighborhood <- as.factor(cases$neighborhood) -->

<!-- return(cases) -->

<!-- } -->

<!-- neighborhood_ranef_plot <- function(zz,pal="Reds") { -->
<!-- ## Point plot of neighborhood random effects -->
<!-- g <- ggplot(zz,aes(x=x,y=y,colour=z, size = activity)) +  -->
<!--   geom_point() +  -->
<!--   scale_colour_distiller(palette=pal,direction=+1) -->

<!-- return(g) -->

<!-- } -->

<!-- neighborhood_mean_plot <- function(zz,pal="Reds") { -->
<!--   ## Point plot of neighborhood random effects -->


<!--   g <- ggplot(zz,aes(x=nx,y=ny,fill=z)) +  -->
<!--     geom_tile() +  -->
<!--     scale_fill_distiller(palette=pal,direction=+1) -->

<!--   return(g) -->

<!-- } -->


<!-- area_data_spatial <- reactive({ -->

<!-- n <- 1000 -->

<!-- df <- autocor_outcome(n, input$alpha_s, input$beta_s, input$rho_s) -->

<!-- return(df) -->

<!-- }) -->

<!-- output$pointSpatialPlot <- renderPlot({ -->

<!--     return(neighborhood_ranef_plot(area_data_spatial())) -->

<!-- }) -->

<!-- output$neighborhoodSpatialPlot <- renderPlot({ -->

<!--     return(neighborhood_mean_plot(area_data_spatial())) -->

<!-- }) -->

<!--  s_model_fit <- reactive({ -->
<!--     if (input$s_adjust == TRUE) { -->
<!--       fit <- lmer(z ~ activity + (1 | neighborhood) , data = area_data_spatial()) -->

<!--     } else { -->

<!--       fit <- lmer(z ~ 1 + (1 | neighborhood) , data = area_data_spatial()) -->

<!--     } -->
<!--     return(fit) -->
<!--   }) -->

<!--   output$s_model_summary <- renderPrint({ -->
<!--     summary(s_model_fit()) -->
<!--   }) -->

<!--   output$pointSpatialResidPlot <- renderPlot({ -->

<!--     df <- area_data_spatial() -->
<!--     df$z <- resid(s_model_fit()) -->

<!--     return(neighborhood_ranef_plot(df,pal="Spectral")) -->

<!-- }) -->

<!-- output$neighborhoodSpatialResidPlot <- renderPlot({ -->

<!--     df <- area_data_spatial() -->
<!--     df$z <- resid(s_model_fit()) -->

<!--     return(neighborhood_mean_plot(df,pal="Spectral")) -->

<!-- }) -->

<!-- ``` -->
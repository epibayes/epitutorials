---
title: "Using Simulation to Better Understand Bayesian and Frequentist Inference"
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
description: "Using Simulation to Better Understand Bayesian and Frequentist Inference"
---

```{r setup, include=FALSE}
library(learnr)
require(rstanarm)
require(bayesplot)
require(ggplot2)
require(plotly)
knitr::opts_chunk$set(echo = FALSE)
```

## Sampling from a Normal Distribution
### Introduction
In this example, we're going to go back to basics, and use both a formula and simulation to calculate confidence intervals for a **sample mean**. 

### Mean and Standard Deviation are Impacted by Sample Size
First, pick a mean and standard deviation and number of samples to draw from a Normal distribution. 

Move the sliders around to get a sense of how much things change as you increase the sample size.

```{r, echo=FALSE}
sidebarPanel(
  numericInput(
    "mean",
    "Mean:",
    value = 0
  ),
  sliderInput(
    "sd",
    "Standard Deviation:",
    min = 0.1,
    max = 4,
    value = 1
  ),
  sliderInput(
    "nsamp",
    "Number of Samples:",
    min = 1,
    max = 500,
    value = 20
  ),
  fluidRow(
  column(12,
         textOutput("samplemean"),
         textOutput("se"),
         style="font-weight: bold;"
         )
)
)
mainPanel(
  plotlyOutput("distPlot2"),
  # plotOutput("distPlot"),
  )

```


```{r, context="server"}

x <- reactive({rnorm(input$nsamp, input$mean, input$sd)})

output$distPlot2 <- renderPlotly({
  plot_ly(x=x(), histfunc= 'avg', type="histogram") %>% layout(yaxis = list(title="Frequency"),bargap = 0.01)
})
output$distPlot <- renderPlot({
  hist(x(),  col = 'darkgray', border = 'white', xlab = mean(x))
})
output$samplemean <- renderText({paste0("Sample Mean = ", round(mean(x()),2))})



output$se <- renderText({paste0("Sample Standard Deviation = ", round(sd(x()),2))})

```

## Calculating Confidence Intervals
### The T-Table Approach
The classic approach to calculating 95% confidence intervals for a population mean is to look up the appropriate value in a **t-table**, as a function of the number of **degrees of freedom**, i.e. the number of overall samples contributing to the mean.

In R, we can do this as follows, where $s$ is the sample standard deviation and $a$ is the sample mean.

```{r, eval=FALSE, echo = TRUE}

## First calculate the standard error using the quantiles of the t-distribution


error <- qt(0.975, df = n-1)*(s/sqrt(n))

## Then, calculate upper and lower confidence limits
lower_ci <- a - error
upper_ci <- a + error

```

<!-- Now, go ahead and calculate these for the input values above: -->

Updating the mean, sample size, and standard deviation from the **"[Sampling from a Normal Distribution"](#section-mean-and-standard-deviation-are-impacted-by-sample-size)** exercise will update the confidence limits below:

```{r, echo=FALSE}
# fluidRow(
#   column(3, actionButton("ci", HTML("Calculate Confidence<br/> Intervals"))),
#   # column(9, verbatimTextOutput("sse", placeholder=TRUE))
#   column(9, plotlyOutput("ssePlot"))
# )

fluidRow(
  column(12, plotlyOutput("ssePlot"))
)

```

```{r, context="server"}

sse <- eventReactive(input$nsamp, {qt(0.975, df = input$nsamp-1)*(sd(x())/sqrt(input$nsamp))})
  

ssePlot <- eventReactive(list(input$nsamp, input$mean, input$sd),{

  df <- data.frame(mean = round(mean(x()),2), lowCI= round(mean(x()) - sse(),2), highCI = round(mean(x())+sse(),2))

g <- ggplot(df)  + geom_errorbar(aes(x = 1, ymin = lowCI, ymax = highCI), size=2) + geom_point(aes(x = 1, y = mean), size=2)
  return(ggplotly(g))
})

output$ssePlot <- renderPlotly({
  return(ssePlot())
})

output$sse <- renderPrint({
  paste0("Sample Mean = ", round(mean(x()),2), " Standard Error = ", round(sse(), 2)," 95% CI = ", round(mean(x()) - sse(),2), ", ",round(mean(x())+sse(),2))
  }
  )
```

### Yeah, But What Does That Mean?

We are supposed to interpret these confidence intervals as meaning "95% of the time, the **true mean** will fall between the confidence intervals. So, why don't we test the proposition?

First, let's take a look at the distribution of **sample means** we get from taking 1000 samples:

```{r, echo=FALSE}

fluidRow(
  column(12, actionButton("sm", "Simulate Sample Means")),
)
fluidRow(
  column(12, plotOutput("samplemeans")),
   style="border: 1px solid #e6e6e6; padding: 10px 0px; margin: 10px 0px;")

```

```{r, context = "server"}

sm_data <- eventReactive(input$sm, {
  mean_output <- rep(0, 1000)
  for (i in 1:1000) {
    mean_output[i] <- mean(rnorm(input$nsamp, mean = input$mean, sd = input$sd))
  }
  
  return(mean_output)

})

output$samplemeans <- renderPlot({hist(sm_data(), xlab = paste0("Standard error of the mean = ", round(1.67*sd(sm_data()),2)))})

```

So, if for each of those sample means, we computed the confidence intervals, if all goes to plan, 95% of the time, they should include the true value of the mean you set all the way at the top. We can do this easily using the `TInterval` function from the R `sigr` package, like so:

```{r, eval=FALSE, echo=TRUE}
for (i in 1:1000) {
    sampled_vals <-rnorm(input$nsamp, mean = input$mean, sd = input$sd)
    tvals <- sigr::TInterval(sampled_vals)
    
    if ((input$mean >= tvals$interval_low) && (input$mean <= tvals$interval_high)) {
      num_containing <- num_containing + 1
    }
}

```

```{r, echo=FALSE}
fluidRow(
  column(3, actionButton("simint", HTML("Simulate Confidence<br/>Intervals"))),
  column(9, verbatimTextOutput("cipct", placeholder=TRUE))
)

```

```{r, context = "server"}

ci_contains <- eventReactive(input$simint,{
  
  num_containing <- 0
  
  for (i in 1:1000) {
    sampled_vals <-rnorm(input$nsamp, mean = input$mean, sd = input$sd)
    tvals <- sigr::TInterval(sampled_vals)
    
    if ((input$mean >= tvals$interval_low) && (input$mean <= tvals$interval_high)) {
      num_containing <- num_containing + 1
    }
    
  }
  
  return(paste0(round(100*num_containing/1000,2), "%"))
  
})


output$cipct <- renderPrint({
  paste0("Contains the true mean ", ci_contains(), " of the time!")
  })

```

## Bayesian Credible Intervals
### Bayes' Rule

Unsurprisingly, the oft-discussed, oft-misunderstood Bayes' rule is at the heart of Bayesian inference. You may be used to the classic formula:

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

For the purposes of inference, we'll re-write it so it looks a bit more familiar:

$$
P(\theta | Y) = \frac{P(Y|\theta)P(\theta)}{P(Y)}
$$
$\theta$ is our **model** and its **parameters** and $Y$ are the data. At this point, you may be thinking, ok great, that's not helpful.  

Let's walk through the four parts of the rule:

1. **Posterior, $P(\theta|Y)$:**  this is the thing we care about estimating. It's a distribution of model parameters (think population means, $\mu$, or the intercept and slope of a regression model), conditioned on the data. In plain English, we might say something like: If these are the data, then these are the parameters that make sense. 

2. **Likelihood, $P(Y|\theta)$:** The likelihood. In other words, given a set of model parameters, what is the likelihood we will observe the data. If our data come from a normal distribution and we are trying to estimate its parameters, this is the probability that $Y \sim \mathcal{N}(\hat{\mu}, \hat{\sigma^2})$, where $\hat{\mu}$ and $\hat{\sigma^2}$ are our guesses at the mean and standard deviation, respectively.

3. **Prior, $P(\theta)$:** This tends to be the big boogeyman. The prior represents our **state of belief** about the model inputs. If we think a particular parameter value is more likely based on prior knowledge, intuition, data, or something else our prior can reflect that. For now, we're not going to worry about it too much, but it's in there.

4. **Probability of the data, $P(Y)$:** We are going to basically ignore this one altogether. Thankfully, it doesn't make much of a difference because it's constant and doesn't depend on our model parameters. 

### Applying Bayes' Rule
Let's imagine we're going to set our prior to be indifferent to the parameter values, i.e. $Pr(\theta) = 1$. All of a sudden, our inference task starts to look a bit less onerous: $Pr(\theta | Y) \propto Pr(Y | \theta)$, where $\propto$ means 'proportional to'. So, to get at our Bayesian estimate, all we need to know is the likelihood! Kind of like in the good-old maximum likelihood example from a few minutes ago. 

Remember, in R, we can write the likelihood of an observed data point from a normal distribution as:

```{r echo=TRUE,eval=FALSE}
y_lik <- dnorm(y, mean = mu, sd = sd)
```


### Estimating Parameters Using the **Posterior**

First, remember the sample mean from the very first example:

```{r}
verbatimTextOutput("sm2")
```

It turns out, that in R, with packages like `rstanarm`, fitting a Bayesian model is roughly as straightforward as fitting a frequentist one, for example, to estimate the mean and standard deviation of our data, we can just write:

```{r, eval=FALSE,echo=TRUE}
library(rstanarm)
fit <- stan_glm(y ~ 1, data = df)
```

Where `df` is a data frame containing our samples under the variable `y`, and the formula `y ~ 1` tells `R` to fit a model with just an intercept, in this case the mean of the sample.


Now go ahead and sample from the posterior and see what you get. In this example, we will just fit the mean, fixing the standard deviation at its input value.

```{r, echo=FALSE}
fluidRow(
  column(12, actionButton("cpost", "Simulate Posterior")),
)
fluidRow(
  column(6,plotOutput("posterior_mean"),),
  column(6, plotOutput("posterior_sigma")), 
  style="border: 1px solid #e6e6e6; padding: 10px 0px; margin: 10px 0px;"
)
```

## Interpreting the Posterior
### Posterior Credible Intervals
While  **confidence intervals** refer to our confidence that the true parameter is between the intervals estimated from **any given sample**, the posterior credible intervals reflect the probability that **the parameter that generated the data** lies between the bounds. 

In other words, in 95% of samples, the parameter that generated the data should lie between the 95% posterior credible intervals.

This may sound a bit like the confidence interval definition, but rather than being a statement about the **sampling distribution** of the data, it is about the model instead.

### How the Posterior Credible Intervals Work
Let's show how this works by borrowing from the thought experiment in this  [great blog post](http://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/):

To understand what the posterior credible intervals are telling us, imagine the following steps:

1. Sample random values of the mean from the prior, i.e. $\mu$.
2. Sample random sets of points $\hat{y}$ given each $\mu$.
3. Keep the values of $mu$ where $\hat{y} = y$.
4. Ask what fraction of the $\mu$ values are within the credible region.

If you were able to do this - a big if, since sampling the ***exact*** set of points is not very likely - then you would find that 95% of the time, the original value of $\mu$ would be between the credible intervals.

Click over to the next topic to see how we can do this in a bit more practical way...

## Sampling From the Posterior
### Using Likelihood to Sample Parameters
Since it is difficult to simulate data that **exactly match** our data, we have to use the **likelihood** to sample parameters at a rate that is **proportional** (there's that word again...) to their likelihood. The classic way to do this is with the [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm). 

### Using the Metropolis-Hastings Algorithm
The steps of the algorithm are as follows:

1. Initialize the starting value of $\mu_0$.

2. Sample the next parameter value, which we'll call $\mu'$ from a **proposal distribution**. We'll use what's called a **random walk proposal distribution**, in which $mu' = \mu_{t} + \epsilon$, where $\epsilon \sim \mathcal{N}(0, \sigma_{w})$ and where $\sigma_w$ is the size of each step. (For the purposes of this example, we'll assume that $\sigma$ is known and we just want to figure out the value of $\mu$.)

The figure below shows a random walk over 500 steps, starting at an initial value of $\mu = 0$:

```{r, warning=FALSE, message=FALSE}
require(ggplot2)
N <- 500
z <- cumsum(rnorm(N,0,1))
df <- data.frame(x = 1:N, y = z)
g <- ggplot(df, aes(x=x,y=y)) + 
  geom_point(size=0.5) + 
  geom_line() + 
  xlab("Step") + 
  ylab("Proposed Value")

ggplotly(g)
```

3. Evaluate the **likelihood ratio** between the last value of $\mu$ and the proposed value, $\mu_i$. We can get this directly from the `dnorm` function in R, by first evaluating the likelihood of the observed data $y$ with respect to both $\mu$ and $\mu'$, like so (in this case assuming a totally non-informative prior for simplicity): 

```{r, echo=TRUE, eval=FALSE}
y_lik_mu <- sum(dnorm(y, mu, sigma, log=TRUE))
y_lik_mu_prime <- sum(dnorm(y, mu_prime, sigma, log=TRUE))
```

We can then put these together to calculate the **acceptance probability** for $\mu'$:

```{r, echo=TRUE, eval=FALSE}
A <- min(1,exp(y_lik_mu_prime - y_lik_mu))
```

What this is saying is that if the ratio of `y_lik_mu_prime` to `y_lik_mu` is < 1, so the proposed value is less likely than the first, we should accept the second one at a rate **proportional to the ratio**. So if it's 0.6, we'll take the 'worse' value 60% of the time. But if it's a 'better' value, i.e. $A > 1$, we will accept it 100% of the time. 

So, now we can put it all together in this bit of code:

```{r, eval=FALSE, echo=TRUE}

steps <- 1000
mu_0 <- 1
step_size <- 0.5
mu_vals <- rep(0, steps)

mu_vals[1] <- mu_0
for (i in 2:steps) {
  
  mu_prime <- mu_vals[i-1] + rnorm(1, 0, step_size)
  
  y_lik_mu <- sum(dnorm(y, mu_vals[i-1], sigma, log=TRUE))
  
  y_lik_mu_prime <- sum(dnorm(y, mu_prime, sigma, log=TRUE))
  
  A <- min(1,exp(y_lik_mu_prime - y_lik_mu))

  if (runif(1) < A) {
    mu_vals[i] <- mu_prime
  } else {
    mu_vals[i] <- mu_vals[i-1]
  }
}
```

Ok, now let's give this a go with some more simulated data:

```{r}
sidebarLayout(
  sidebarPanel(
    sliderInput("mu2", "Mean:", min = -10, max = 10, value = 2, step = 0.1),
    sliderInput("sigma2", "SD:", min = 0.2, max = 10, value = 1, step = 0.1 ),
    sliderInput("nsamp2", "Number of Samples:", min = 10, max = 500, value = 30, step = 1)
  ),
  mainPanel(
    plotlyOutput("y2plot")
  )
)
```

Now, let's sample using the Metropolis-Hastings algorithm and see what we get:

```{r}
sidebarLayout(
  sidebarPanel(
    sliderInput("stepsize", "Step Size:", min = 0.1, max = 3, value = 1, step = 0.1),
    sliderInput("numsteps", "Number of MCMC Steps:", min = 1000, max = 5000, value = 2000, step = 100),
    # actionButton("runmcmc", "Run MCMC")
  ),
  mainPanel(
    plotlyOutput("mcmcfigure")
  )
)
```

```{r, context = "server"}

output$sm2 <- renderText({
  paste0("Sample Mean = ", round(mean(x()),2))
  })

bayes_fit <- eventReactive(input$cpost, {
  fit <- stan_glm(y ~ 1, data = data.frame(y = x()))

})

output$posterior_mean <- renderPlot({
    posterior <- as.matrix(bayes_fit())

  plot_title <- ggtitle("Posterior Mean",
                      "With Medians and 95% Interval")
  mcmc_areas(posterior, 
           pars = c("(Intercept)"), 
           prob = 0.95) + plot_title + geom_vline(xintercept = input$mean)
})

output$posterior_sigma <- renderPlot({
    posterior <- as.matrix(bayes_fit())

  plot_title <- ggtitle("Posterior Standard Deviation",
                      "With Median and 95% Interval")
  mcmc_areas(posterior, 
           pars = c("sigma"), 
           prob = 0.95) + plot_title + geom_vline(xintercept = input$sd)
})

output$posteriorEst <- renderText({
  z <- quantile(rnorm(100), probs=c(0.025, 0.5, 0.975))
  
  
zz <- paste0("Posterior Median = ", round(z[2],2), "; Posterior Credible Intervals = ", round(z[1],2),"-", round(z[3],2))
return(zz)

  })

mh_samples <- function(y, sigma, steps, burnin, step_size) {
  
total_steps <- burnin+steps

mu_0 <- 1
mu_vals <- rep(0, total_steps)

mu_vals[1] <- mu_0
for (i in 2:total_steps) {
  

  mu_prime <- mu_vals[i-1] + rnorm(1, 0, step_size)
  
  y_lik_mu <- sum(dnorm(y, mu_vals[i-1], sigma, log=TRUE))
  

  y_lik_mu_prime <- sum(dnorm(y, mu_prime, sigma, log=TRUE))
  
  A <- min(1,exp(y_lik_mu_prime - y_lik_mu))

  if (runif(1) < A) {
    mu_vals[i] <- mu_prime
  } else {
    mu_vals[i] <- mu_vals[i-1]
  }
}

return(mu_vals[burnin:total_steps])
  
}

bayes_y <- reactive({
  y <- rnorm(input$nsamp2, input$mu2, input$sigma2)
})

output$y2plot <- renderPlotly({
  g <- ggplot(data.frame(y=bayes_y())) + geom_histogram(aes(x=y))
  p <- ggplotly(g)
  return(p)
})

mcmc_fit <- eventReactive(list(input$sigma2, input$numsteps,input$stepsize,input$nsamp2, input$mu2, input$sigma2),{
  y <- mh_samples(bayes_y(), input$sigma2, input$numsteps, 1000, input$stepsize)
  return(y)
})

output$mcmcfigure <- renderPlotly({
  qvals <- quantile(mcmc_fit(), probs = c(0.025, 0.5, 0.975))
    g <- ggplot(data.frame(y=mcmc_fit())) + geom_histogram(aes(x=y)) + geom_vline(xintercept = qvals[2])
    p <- ggplotly(g)
  return(p)
})

output$mcmctrace <- renderPlot({
  
})
```

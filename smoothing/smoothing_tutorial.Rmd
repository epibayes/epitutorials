---
title: "Smoothing!"
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
require(learnr)
require(ggplot2)
require(plotly)
require(lhs)
require(FNN)
require(truncnorm)
require(waiter)
knitr::opts_chunk$set(echo = FALSE)
```

## What is Smoothing?
### A Brief Introduction to Smoothing
Often, the task of modeling - spatial and otherwise - is focused on understanding what happens **between** the data points we are able to observe. Just like Lisa Simpson reminds us that jazz is in the space between the notes, smoothing happens in between the data points.

<div style="width: 100%; height: 40vh;">
<iframe width="100%;" height="100%;" src="https://www.youtube.com/embed/BbeilmP2wY8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

### Smoothing is a Tool
In spatial analysis, smoothing is a key tool that is used in a number of ways:

1. **Visualization**: A bunch of points on a map are hard to interpret, so tools that make them into a **continuous surface** conveying some sort of information are much easier to digest.

2. **Inference**: We often care about finding areas of elevated or decreased risk, so-called hot and cold spots. Spatial smoothing allows us to highlight areas in which risk of some outcome is elevated across multiple observations.

3. **Prediction**: Our observed data may represent only a subset of the population. In this case, we can use smoothing to say something more about what might be going on between the observed data points in a way that might guide future interventions and/or data collection.

### Tutorial Overview
In this tutorial, we will introduce some key concepts and tools for smoothing and visualizing potentially non-linear data. 

We will focus on **local regression** techniques for continuous outcomes, e.g. BMI, blood pressure, etc, in in one dimension, e.g. in response to some exposure, and in two dimensions representing x and y spatial coordinates. 

In a future tutorial we will look at complementary approaches to **spatial density estimation** which let us estimate the probability of an event occurring in space that build on the concepts discussed in this tutorial.

## Smoothing in One Dimension
### Generate Data
Our first task will be to generate some data that we'll use as input to our smoother from a linear model of the classic sort, i.e. $y_i \sim Normal(\alpha + \beta x_i, \sigma^2)$:

```{r, echo=FALSE}
sidebarPanel(
  use_waiter(),

  sliderInput("beta", "Slope:", min = 0.1, max = 5, value = 2),
  sliderInput("alpha", "Intercept:", min = 0.1, max = 5, value = 0.1),
  sliderInput("sd", "Error Standard Deviation:", min = 0.1, max = 4, value = 1)
)

mainPanel(
  plotlyOutput("simDataPlot")
)

```

The overlaid red line is the **best-fit regression line** obtained from the least-squares regression function in R, i.e. `lm(y ~ x, data = df)`. Nothing too surprising here. 

But what happens when we allow our data to be **non-linear**, coming from something like a sine function? 

Then, our formula looks something like $y_i \sim Normal(\alpha + A*sin(F*x_i)), \sigma^2$, where $A$ is the **amplitude** of the sine function, and $F$ is its **frequency**. 

Try moving the sliders around below and see what happens to the simulated data above. The panel below illustrates what the underlying sine function looks like without any noise:

```{r}
sidebarPanel(
  sliderInput("period", "Frequency", min = 0.0, max = 10.0, value = 0.0),
  sliderInput("amp", "Amplitude", min = 0.1, max = 3, value = 1)
)

mainPanel(
  plotlyOutput("sinePlot")
)
```

```{r, context="server"}
xvals <- seq(from = 0.0, to = 10, by  = 0.05)
sim_data <- reactive({
  y <- rnorm(length(xvals), input$alpha + input$beta*xvals + input$amp*sin(xvals*input$period), input$sd)
  
  return(data.frame(x = xvals, y = y))
})


model_fit <- reactive({
  fit <- loess(y ~ x, data = sim_data(), span = input$smooth, degree = input$smoother_degree)
  return(fit)
})


output$sinePlot <- renderPlotly({
  
  df <- data.frame(x = xvals, y = input$amp*sin(xvals*input$period))
  
  g <- ggplot(df, aes(x = x, y = y)) + 
    geom_line() +
    xlim(0, 10) +
    ylim(-4, 4)
  
  return(ggplotly(g))
  
})

output$simDataPlot <- renderPlotly({
  
  df <- sim_data()
  df$pred <- predict(lm(y ~ x, data = df))
  
  g <- ggplot(df, aes(x=x)) +
    geom_point(aes(y=y), size = 1) + 
    geom_line(aes(y = y), size = 1, colour = "gray") +
    geom_line(aes(y = pred), color="red") +
    xlim(0, 10) 
  
  return(ggplotly(g))
  
})


xpt <- reactiveVal(value = NA)

observeEvent(input$mp_click, {
  np <- nearPoints(sim_data(), input$mp_click, "x", "y", threshold = 30, maxpoints = 1)
  if(nrow(np) == 0) {
    xpt(NA)
  } else {
    xpt(np$x)
  }
})

data_w_model <- reactive({
  df <- sim_data()
  mf <- model_fit()
  
  df$pred <- predict(mf)
  df$resid <- resid(mf)
  
  return(df)
})

output$residPlot <- renderPlot({
  hist(data_w_model()$resid)
})



output$distPlot <- renderPlot({
  
  ## Unload the data into local var
  model_d <- data_w_model()
  model_d$selected <- 0.1
  model_d$weight <- 0
  
  model_d$color <- "black"
  if (is.na(xpt()) == FALSE) {
    ## Find nearest neighbors of point
    xpt_idx <- match(xpt(), model_d$x)
    neighbor_dist <- knn_vals()$nn.dist[xpt_idx,]
    max_dist <- max(neighbor_dist)
    scaled_dist <- neighbor_dist/max_dist
    
    
    neighbor_idx <- knn_vals()$nn.index[xpt_idx,]
    model_d$selected[neighbor_idx] <- 1
    model_d$weight[neighbor_idx] <-  (1-abs(scaled_dist)**3)**3
    model_d$color[neighbor_idx] <- "red"
    model_d$selected[xpt_idx] <- 1
    model_d$weight[xpt_idx] <- 1
    model_d$color[xpt_idx] <- "blue"
    
  }
  
  ## Get weights for each point
  fit_d <- model_d %>% filter(selected == 1) 
  if (input$smoother_degree == 0) {
    f <- y ~ 1
  } else if (input$smoother_degree == 1) {
    f <- y ~ x 
  } else {
    f <- y ~ poly(x,2)
  }
  
  mm <- lm(f, data = model_d, weights = model_d$weight)
  
  model_d$subset_pred <- predict(mm)
  
  g <- ggplot(model_d, aes(x=x))
  
  g <- g + 
    geom_line(aes(y = y), size = 1, colour = "gray") + 
    geom_point(aes(y=y), size = 3 + model_d$weight, colour = model_d$color) +
    geom_line(aes(y=pred), size = 1, color="red")

  
  if (is.na(xpt()) == FALSE) {
    g <- g+ geom_vline(xintercept = xpt(), colour ="gray", size = 1.5) + 
      geom_line(aes(y=subset_pred), colour = "blue", linetype ="dashed", size = 1.5)
  }
  
  g <- g +  xlim(0, 10) + ylim(min(model_d$y)-2, max(model_d$y)+2)

  return(g)
  
})


output$model_summary <- renderPrint({
  summary(model_fit())
})


knn_vals <- reactive({
  kk = ceiling(input$smooth*length(xvals))
  if (kk >= length(xvals)) {
    kk <- length(xvals)-1
  }
  get.knn(xvals, k = kk)
})

```

### Beyond Linear Models

Now, we can start looking at what happens when we fit some kind of model to our data. Here, the figure shows the original data, with the predictions of a simple linear model predicting y as a linear function of x overlaid. 

Take a look at the **non-linear** model below. 

A **kernel smoother** or **kernel regression model** is a commonly-used type of smoother and an example of a non-linear model. 

In a kernel smoother, a subset of points near to the point of interest as used to estimate the value of the unobserved smooth function of interest.

What a kernel function does is to **weigh points based on their distance from the point of interest**. In the simplest version of smoothing, we can use this to take a weighted average. 

So, if $h$ is the **bandwidth** of the smoother, in this case defined as the proportion of all the observed points used to estimate the function at each point, we can define a kernel function $f(d|h)$ which returns a weight from 0 to 1 for each point in the space as a function of its distance, $d$, from the point of interest. The figure below shows what this type of weighted moving average looks like for varying values of $h$.

For continuous outcomes, like systolic blood pressure, a commonly used kernel function is the **tri-cube kernel**, $w(x) = (1- |d|^3)^3$, where $d$ is the distance from the point of interest, scaled to fall on the range from 0 to 1, where 1 is the maximum distance within the bandwidth of the kernel. So, for a maximum distance of 1, it looks like this:

```{r}
x <- seq(from = -1, to = 1, by = 0.01)
y <- (1-abs(x)**3)**3
df <- data.frame(x = x, y = y)
g <- ggplot(df, aes(x=x, y=y)) + geom_point() + xlab("Distance") + ylab("Weight")
ggplotly(g)
```

When you click on the plot, it will highlight the points used for interpolation at the selected point in red, and highlight the point itself in blue. 

```{r}
sidebarPanel(
  sliderInput("smooth", "Proportion of points used for smoothing", min = 0.01, max = 1.0, value = 0.2),
  selectInput("smoother_degree", "Smoother Type:",
              c("Moving Average" = 0,
                "Linear" = 1,
                "Polynomial" = 2))
)

mainPanel(
  tabsetPanel(
    tabPanel("Plot", 
             plotOutput("distPlot", click = "mp_click")),
    tabPanel("Residuals", plotOutput("residPlot")),
    tabPanel("Model Summary", verbatimTextOutput("model_summary"))
  )
)
```

One thing you may notice is that here we are use the **k** nearest neighbors of the point, where $k \approx hN$, and N is the total population size. As you may be able to see, this leads to particular difficulties at the edges of the figure, where higher and lower values skew the estimates, respectively. 

One way to deal with this is to fit a **weighted regression model** that accounts for change at each point by fitting a linear regression model of the form `lm(y ~ x, data = df, weights = w)`, which tells the model to count closer points more than far ones. 

It is also common to use a **polynomial function**, i.e. `lm(y ~ poly(x,2), data  =df, weights = w)` which fits a model of the form $y = a + bx + cx^2$ to the data, to approximate each point. Try selecting the linear and polynomial options in the box above and see what happens to the estimate of the function. The blue dashed line shows the predictions of the weighted regression model at the selected point.

#### Questions
1. What happens with all of these approaches as you increase the frequency of the function or its amplitude?

2. Is there a point where increasing the **observation noise** washes out any potential non-linear effects you might pick up?

## Smoothing in Two Dimensions
### Thinking About **Density**

In spatial analysis, we often want to know whether a particular area has more **cases** than we might expect by chance alone. 

Smoothing can be a powerful tool both for completing exploratory visualizations of our data to highlight areas of concern, and to confirm whether or not the patterns we detect visually are likely to be robust. 

In the next few steps, we'll explore how **kernel density estimation (KDE)** can be used to visualize the distribution of individuals in space, as well as how we can use KDE to measure the **intensity** of risk in a given area. 

### What is **Kernel Density Estimation**?

Compared to **kernel smoothing** from the previous example, KDE is used to **approximate the distribution of some random variable** that may have an unknown distribution, i.e. we're not sure if the data came from a Normal distribution or some other statistical distribution we can write down the parameters for. 

KDE is often useful for spatial analysis because we often want to measure the **probability of observing a case** at a given location, but don't know exactly what the statistical process that generated these data are. So, we can use a smoother to estimate the probability that a case will appear at any given location on the map. 

**Using the smoother lets us estimate this probability not only for the areas where we see cases, but also for nearby areas as well.** And this is the reason these tools are so useful for making comparisons, as we'll see in a moment.

### Spatial **Case-Control** Analysis

A common design for studies in epidemiology is a **case-control** design in which we compare the attributes of cases to those of a randomly-selected set of controls (i.e. non-cases) to figure out how cases and controls differ, e.g. by age, sex, socioeconomic status, etc. 

We can use this approach for spatial analysis as well, where the question we want to answer is: **is the spatial distribution of cases different than the spatial distribution of controls?** 

In other words, are cases **more likely** to appear in certain places, which may be suggestive of the **clustering of risk** in that area? 

We can then use smoothing tools like KDE to compare the proportion of cases that show up in a given location with the proportion of controls, and where that difference is large we often say that we have found a **hotspot**, i.e. a place where there are many more cases than controls. 

### Test-Drive
To get a feel for what this looks like, let's generate some simple spatial **case-control** data. 

For starters, let's just assume that the **underlying population density is the same everywhere**. 

In this model, we will have two processes: 

1. One placing disease-free controls **randomly in space**, and 

2. One representing the **distribution of cases**. 

By shrinking the radius of the case distribution, you make the cases more concentrated in a given location, resulting in more of a **hotspot**. As the radius grows, the distribution of cases should look more like the distribution of controls:

```{r, echo=FALSE}
fluidRow(
  column(12,
         sidebarPanel(
  sliderInput("n", "Total Controls:", min = 500, max = 5000, value = 1000),
  sliderInput("nc", "Total Cases", min = 100 , max = 1000, value = 100),
  sliderInput("hr", "Case 'hotspot' Radius", min = 0.1, max = 5, value = 1.5),
  checkboxInput("points", "Show Case and Control Locations", TRUE)
),
mainPanel(
  plotOutput("dataPlot")
)
         )
)

```

```{r, context="server"}
case_data <- reactive({
  
  ## Generate points on a 10x10 square
  num_bg_pts <- input$n
  background_pts <- data.frame(x = runif(num_bg_pts, 0, 10), 
                               y = runif(num_bg_pts, 0, 10))
  background_pts$case <- 0
  hotspot_center <- 5
  hotspot_sd <- 0.25
  
  ## Add in a hotspot centered in the middle
  num_hs_pts <- input$nc
  hotspot_pts <- data.frame(x = rtruncnorm(num_hs_pts, a= 0, b= 10, hotspot_center, input$hr),
                            y = rtruncnorm(num_hs_pts, a = 0, b = 10,hotspot_center, input$hr))
  hotspot_pts$case <- 1
  
  out_df <- rbind(background_pts, hotspot_pts)
  
  return(out_df)
})


output$dataPlot <- renderPlot({
  
  g <- ggplot(case_data(), aes(x=x, y = y)) + geom_density2d_filled()
  if (input$points == TRUE) {
    g <- g + geom_point(aes(colour=case), size=2)
  }
  
  g <- g + coord_equal() + theme(legend.position = "none") 
  return(g)
  
})

output$smoothAll <- renderPlot({
  g <- ggplot(case_data(), aes(x=x,y=y)) + 
    geom_density2d_filled() + theme(legend.position = "none")
  return(g) 
})

output$smoothCC <- renderPlot({
  g <- ggplot(case_data(), aes(x=x,y=y)) + 
    geom_density2d_filled() + theme(legend.position = "none") + facet_wrap(~case)
  return(g) 
})

random_case_data <- reactive({
  df2<- case_data()
  df2$case <- sample(df2$case)
  return(df2)
})

output$smoothCCrandom <- renderPlot({
  
  g <- ggplot(random_case_data(), aes(x=x,y=y)) + 
    geom_density2d_filled() + geom_point(size=2) + theme(legend.position = "none") + facet_wrap(~case)
  return(g) 
})

output$diffPlotRandom <- renderPlotly({
  
  df <- random_case_data()
  hotspot_pts <- df %>% filter(case == 1)
  background_pts <- df %>% filter(case == 0)
  
  ## Get differences between case and control
  cs_smooth <- kde2ddf(hotspot_pts)
  cl_smooth <- kde2ddf(background_pts)
  cs_smooth$z<- cs_smooth$p - cl_smooth$p
  g <- ggplot(cs_smooth, aes(x=x,y=y,z=z)) + geom_raster(aes(fill = z)) + coord_equal() +  theme(legend.position = "none")
  return(ggplotly(g))
})

output$diffHistRandom <- renderPlot({
  df <- random_case_data()
  hotspot_pts <- df %>% filter(case == 1)
  background_pts <- df %>% filter(case == 0)
  
  cs_smooth <- kde2ddf(hotspot_pts)
  cl_smooth <- kde2ddf(background_pts)
  cs_smooth$z<- cs_smooth$p - cl_smooth$p
  
  return(hist(cs_smooth$z, breaks=100))
  
})
output$diffHistRandomPlotly <- renderPlotly({
  df <- random_case_data()
  hotspot_pts <- df %>% filter(case == 1)
  background_pts <- df %>% filter(case == 0)
  
  cs_smooth <- kde2ddf(hotspot_pts)
  cl_smooth <- kde2ddf(background_pts)
  cs_smooth$z <- cs_smooth$p - cl_smooth$p
  
  g <- ggplot(data.frame(x = cs_smooth$z)) + geom_histogram(aes(x=x), bins=100)+ xlab("cs_smooth$z")+ ylab("Frequency")
  p <- ggplotly(g)
  return(p)
  
})
```

#### Try to Find the Hotspots
Try some different values and see when it's hard to see the hotspot with the naked eye when it's not. Use the toggle to turn on coloring for the cases to see how good your intuition is.

### Do the Cases and Controls Have the Same Distribution?

We can go one step further and ask whether the cases and controls seem to have the same distribution or not by separating them out and **plotting the spatial density of cases and controls**, which you can see in the plot below:


```{r, echo=FALSE}

fluidRow(
    column(12,  plotOutput("smoothCC")),
)

```

```{r, context="server"}
kde2ddf <- function(df, n = 50, lims = c(range(0:10),range(0:10))){
  require(MASS)
  ## This code turns the data into a df
  dd <- kde2d(df$x, df$y, n= 100, lims = lims)
  gr <- data.frame(with(dd, expand.grid(x,y)), as.vector(dd$z))
  colnames(gr) <- c("x","y","z")
  gr$p <- gr$z/sum(gr$z)
  return(gr)
}

data_diff <- reactive({
  df <- case_data()
  hotspot_pts <- df %>% filter(case == 1)
  background_pts <- df %>% filter(case == 0)
  
  ## Get differences between case and control
  cs_smooth <- kde2ddf(hotspot_pts)
  cl_smooth <- kde2ddf(background_pts)
  cs_smooth$z<- cs_smooth$p - cl_smooth$p
  
  return(cs_smooth)
})

output$diffPlot <- renderPlotly({
  df <- data_diff()
  g <- ggplot(df, aes(x=x,y=y,z=z)) + geom_raster(aes(fill = z)) + coord_equal() + theme(legend.position = "none")
  return(ggplotly(g))
})


```

We can ask this question a bit more formally by subtracting the **density of cases** at each point on the grid from the **density of controls**. 

In places where the probability of observing a case is much greater than observing a control, we should see a positive difference. But if there is no difference, the value should be close to zero. 

The figure below shows what happens when you subtract the **control** density above from the **case** density:

```{r, echo=FALSE}
fluidRow(
  column(12,  plotlyOutput("diffPlot"),)
)
```

Depending on the value of the sliders you set above, you might see a small or medium-sized region of increased risk, or if the radius of the case distribution is wide enough, you may see essentially no difference. But if we **do** see a pattern, how can we know whether it is actually meaningful?

### Complete Spatial Randomness

This is where the concept of **complete spatial randomness (CSR)** becomes useful. 

To figure out whether our pattern is reflective of more than just random noise, we can ask the question of whether **what we see** - in this case the spatial distribution of cases vs. controls - **differs** in any meaningful sense from a world in which the locations of the points in our dataset stay the same, but their status as cases and controls is scrambled. 

That way, when we separate out cases and controls and look at their spatial densities, we should see similar patterns, like the plots below:

```{r, echo=FALSE}
fluidRow(
  column(12,  plotOutput("smoothCCrandom")),
)

```

```{r, echo=FALSE}
fluidRow(
  column(12, plotlyOutput("diffPlotRandom"))
)
```

If we take a look at the histogram of these differences across all the locations, we can see that the mean is pretty close to zero, but there is a lot of variation:

```{r, echo=FALSE}
fluidRow(
  column(12, plotOutput("diffHistRandom")),
  column(12, plotlyOutput("diffHistRandomPlotly"))
)
```

In the final section, we'll take advantage of this random variation to ask how robust the pattern we generated all the way back at the top actually is!

### A Permutation Test

To assess how robust our results are, we need to formalize our question into something like a statistical hypothesis. For this example, we're going to ask this question:

"Which differences between case and control densities in our data are greater than 95% of the **largest differences** from 1000 random permutations of the case and control labels in our data."

In other words, when you click the button below, we'll complete the shuffling step above 1000 times, calculate the largest difference for each one of these samples, and then highlight areas on our map where the difference is larger than almost all of the most extreme differences obtained randomly:

```{r, echo=FALSE}
fluidRow(column(3,  actionButton(
  "simulate", HTML("Draw Random<br/> Permutations!")
)),
column(9,
       tabsetPanel(
         tabPanel("Max Values", plotlyOutput("maxValsHistPlotly")),
         tabPanel("Hotspots", plotOutput("hotspotPlot")),
         tabPanel("Percents", textOutput("dataPVal"))
       )))

```

```{r, context="server"}
  waitress <- Waitress$new("#simulate", theme = "overlay", infinite = TRUE)

  randomDist <- eventReactive(input$simulate, {
    rdf <- case_data()
    maxvals <- rep(0, 100)
    waiter_show( # show the waiter
      html = spin_fading_circles() # use a spinner
    )
    for (i in 1:500) {
      rdf$case <- sample(rdf$case)
      cs_smooth <- rdf %>% filter(case == 1) %>% kde2ddf()
      cl_smooth <- rdf %>% filter(case == 0) %>% kde2ddf()
      cs_smooth$z<- cs_smooth$p - cl_smooth$p
      maxvals[i] <- max(cs_smooth$z)

    }
    waiter_hide()
    
    return(list(max = maxvals, data = data_diff()))

  }, ignoreNULL = TRUE, ignoreInit=TRUE)
  
  # output$maxValsHist <- renderPlot({
  #   dd <- randomDist()
  #   df <- data.frame(x = dd$max)
  #   g <- ggplot(df, aes(x)) + geom_histogram()
  #   return(g)
  # })
    output$maxValsHistPlotly <- renderPlotly({
    dd <- randomDist()
    df <- data.frame(x = dd$max)
    g <- ggplot(df, aes(x)) + geom_histogram()
    return(ggplotly(g))
  })
  
  randomEcdf <- reactive({
    z <- randomDist()
    return(ecdf(z$max))
  })
  
  output$dataPVal <- renderText({
    z <- randomDist()
    dd <- randomEcdf()
    p <- dd(z$data$z)
    df <- z$data
    df$p <- p
    df$hot <- 0
    df$hot[df$p >= 0.95] <-1
    pct_hot <- round(100*sum(df$hot)/nrow(df),0)
    
    return(paste0(pct_hot, "% of locations have more cases than expected by chance"))
  })
  
  output$hotspotPlot <- renderPlot({
        waitress$start()

    z <- randomDist()
    datadf <- case_data()
    dd <- randomEcdf()
    p <- dd(z$data$z)
    df <- z$data
    df$p <- p
    df$hot <- 0
    df$hot[df$p >= 0.95] <-1
    
    g <- ggplot() + geom_raster(data=df,aes(x=x,y=y,fill=hot)) + geom_point(data=datadf,aes(x=x,y=y, shape=as.factor(case)), size=2, color="lightgrey")
        waitress$close()
    return(g)
    
  })
  
```

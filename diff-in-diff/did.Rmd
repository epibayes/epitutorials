---
title: "Using Spatial Data to Estimate the Effects of Treatment"
output: 
  learnr::tutorial:
    progressive: true
    # allow_skip: true
runtime: shiny_prerendered
description: "Use Spatial Data to Estimate the Effects of Treatment"
---

```{r setup, include=FALSE}
require(learnr)
require(ggplot2)
require(plotly)
require(lhs)
require(FNN)
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction
### Making Comparisons Between Places is Hard 

One of the big challenges of real-world epidemiology is in disentangling the effects of some exposure or treatment we want to learn about from the background noise that makes different places… different. These differences come in many forms, including:

1. Variation in **socioeconomic status (SES)** and **social power**.
2. Variability in **characteristics of the built environment** that might impact the health outcome of interest.
3. Differential **access to prevention, diagnosis, and care** which way lead to differential disease outcomes among individuals living in different places.

### Randomized Controlled Trials: A Potential Approach
One way to deal with these differences is to conduct a randomized controlled trial (RCT). In a ‘pure’ spatial RCT, individuals might be allocated at random to two different locations which are the same, except that people in one place receive the treatment and those in the other do not. Because the composition of each place - in terms of individual characteristics - is roughly the same due to the randomization, we can then look at the average difference between the groups to estimate the impact of the treatment on a typical member of the population.

Of course, conducting such an experiment is pretty much impossible, probably unethical, and is also unlikely to yield us much useful information about how a particular treatment or exposure might work in the real world. 

Often, when we want to figure out how an exposure or intervention applied in a given place has - or has not - worked, we have to use analytic approaches that let us isolate the changes we are interested in from those resulting from unobserved characteristics of the place itself or the people in it that are unrelated to the ones we care about. 

### The Difference-in-Differences Approach
In this exercise, we will explore one commonly-employed way of doing this, the **Difference-in-Differences (DiD)** approach. The purpose of this module is not only to introduce you to the DiD approach to estimating treatment and exposure effects from non-randomized, observational data, but also to help you think about how and when such tools are appropriate and also when they might *not* yield useful information.

## Using Experimental Data
### A Hypothetical Trial: Pre-Treatment Outcomes

Imagine we are running and randomized controlled trial in which individuals are sampled at random to be placed in the **experimental** and **control** groups. In the population, there are high and low risk individuals, but they are sampled into the two groups uniformly at random. 

The sliders below let you set the average value of the pre-treatment outcome, the pre-treatment difference between the control and experimental groups, and the proportion of the population in the high-risk group.

```{r, echo = FALSE}
fluidRow(
  column(12, plotlyOutput("exp_model_plot"))
)
fluidRow(
  column(6,
         sliderInput(
      "alpha",
      "Pre-Treatment Average:",
      min = 0,
      max = 20,
      value = 10
    ),
    sliderInput(
      "beta_t",
      "Treatment Effect",
      min = -10,
      max = 0,
      value = -5
    ),
    sliderInput(
      "sd",
      "Error Standard Deviation:",
      min = 0.1,
      max = 2,
      value = .5
    )),
  column(6,
         sliderInput(
      "p_high",
      "Proportion High-Risk:",
      min = 0.1,
      max = .5,
      value = .25
    ),
    sliderInput(
      "alpha_diff",
      "Pre-Treatment Difference Between High and Low-Risk Groups",
      min = 5,
      max = 20,
      value = 8
    ),
    sliderInput(
      "N",
      "Number of Observations",
      step = 2,
      min = 100,
      max = 1000,
      value = 500
    ))
)
fluidRow(
  column(12, tags$details(tags$summary("Data Summary", style="font-weight: 18px; font-weight: bold; padding: 10px;"), verbatimTextOutput("exp_model_summary"), style="background-color: #ecf3f9"))
)

# sidebarLayout(
#   sidebarPanel(
#     sliderInput(
#       "alpha",
#       "Pre-treatment average:",
#       min = 0,
#       max = 20,
#       value = 10
#     ),
#     sliderInput(
#       "beta_t",
#       "Treatment Effect",
#       min = -10,
#       max = 0,
#       value = -5
#     ),
#     sliderInput(
#       "sd",
#       "Error Standard Deviation:",
#       min = 0.1,
#       max = 2,
#       value = .5
#     ),
#     sliderInput(
#       "p_high",
#       "Proportion high-risk:",
#       min = 0.1,
#       max = .5,
#       value = .25
#     ),
#     sliderInput(
#       "alpha_diff",
#       "Pre-treatment difference between high and low-risk groups",
#       min = 5,
#       max = 20,
#       value = 8
#     ),
#     sliderInput(
#       "N",
#       "Number of observations",
#       step = 2,
#       min = 100,
#       max = 1000,
#       value = 500
#     ),
#   ),
#   mainPanel(plotOutput("exp_model_plot"))
# )
# 
# fluidRow(column(12,
#                 verbatimTextOutput("exp_model_summary"))
# )


```

In this scenario, high and low-risk individuals are allocated to the different groups at random, and as a result, the **average treatment effect** estimated in the regression model is correct. 

However, for most problems involving some kind of **spatial exposure**, e.g. to a neighborhood environment, it is either unethical, impractical, or impossible to randomize people to different neighborhoods. 

So, what happens when high- or low-risk individuals are over-represented in one group vs. another?

### What happens when the groups aren't **balanced**?

Up to this point, we've focused on the situation in which the high and low risk individuals are evenly divided among the groups. 

But what if we want to look at the impact of living in neighborhood A vs. neighborhood B on some health outcome, e.g. systolic blood pressure? 

Move the slider around below to increase the odds that low-risk individuals (i.e. those with lower starting blood pressure) have chosen to move to the healthier neighborhood. Open the "Data Model Plot" dropdown to see how changing the odds impacts treatment.

```{r, echo=FALSE}

fluidRow(
  column(6, 
         sliderInput("or_hr", "Odds that Low-Risk Individuals Live in Treatment Neighborhood (vs. High-Risk Individuals):", min = 1.0, max = 10.0, value = 1.0, animate = FALSE),),
  column(6,
           plotlyOutput("selectionError"),)
)

fluidRow(
  column(12, tags$details(tags$summary("Data Model Plot", style="font-weight: 18px; font-weight: bold; padding: 10px; background-color: #ecf3f9;"), plotlyOutput("exp_model_plot2")))
, style="padding-bottom: 10px;")

fluidRow(
  column(12, tags$details(tags$summary("Data Summary", style="font-weight: 18px; font-weight: bold; padding: 10px;"), verbatimTextOutput("exp_model_summary2"), style="background-color: #ecf3f9"))
)

```

The range in the figure shows the estimated treatment effect and the point shows its true value. 

What happens to our estimates of this effect as low-risk individuals are increasingly likely to live in the treatment neighborhood?

## Observational Data in Analyses 

The situation in the last step, where it is difficult to disentangle variability in outcomes caused by **selection** into the treatment vs. control groups is an inherent problem with spatial data. Since our analyses typically rely on **observational** data, the problem of isolating actual treatment effects is both important...and challenging.

This is often done via some kind of pre-post comparison: Take blood pressure measurements from individuals in the treatment and control groups before the institution of some policy or intervention, then take their blood pressure after the fact. So, first go ahead and generate some data, and then we'll discuss how to isolate the treatment effect from them.

### Generate Some Data

The figure below shows the observational version of our example from before. On the left, are the **pre-treatment** measurements of the control and experimental groups (colors represent risk groups, symbols represent treatment vs. control).

Now, in addition to having a treatment effect at time 1, we have to account for possible sources of change in the outcome **unrelated to the treatment** between time 0 (treatment) and time 1 (post-treatment). 

Move the slider below to modify the change in background risk impacting both the experimental and control groups to see what happens to the pre-and post-treatment data in the figure below.

The dashed line in the figure shows the estimated pre- and post-treatment values of the outcome for both groups. If you look at the regression results, you can see something here is not quite right:

```{r, echo=FALSE}
fluidRow(
  column(3,
         sliderInput(
            "beta_time",
            "Time Effect:",
            min = -5,
            max = 5,
            value = 3           
         )),
  column(9,
         plotlyOutput("simDataPlot"))
)

fluidRow(
  column(12, tags$details(tags$summary("Data Summary", style="font-weight: 18px; font-weight: bold; padding: 10px;"), verbatimTextOutput("model_summary"), style="background-color: #ecf3f9"))
)

```

This is because the estimated treatment effect in this model is based on a comparison between the **treated** and **untreated** at time 1, but it can't tell the difference between the change over time and the treatment.

### The Difference-in-Differences Model Helps!

That is where the **difference-in-differences** model becomes helpful: This model adds an interaction between the **time** and the **treatment** variable, so:

$\hat{y} = \alpha + \beta_{time} x_{time} + \beta_{treat} x_{treat} + \beta_{treat \times time} x_{treat} x_{time}$

The interaction term gives the specific effect of the **treatment on the treated**.

Check the box below to fit this model to the data and take a look at how the estimates in the figure and regression model change.

```{r, echo=FALSE}
fluidRow(
  column(3,
  checkboxInput("did", "Difference-in-Difference", value=FALSE)
  ),
  column(9,
  plotlyOutput("simDataPlot2")
  ),
)
fluidRow(
  column(12, tags$details(tags$summary("Data Summary", style="font-weight: 18px; font-weight: bold; padding: 10px;"), verbatimTextOutput("model_summary2"), style="background-color: #ecf3f9;"))
)
```

Now, the red line illustrates the true treatment effect, the solid and dotted lines show the average change over time for the treatment and control groups, respectively. 

The **dotted** line shows what the value of the outcome for the treatment group **would have been** in the absence of intervention. 

This is where the notion of difference in differences comes from: We use information on the expected difference from time 0 to time 1 from the control group to **estimate what the outcome would look like** for the treatment group if there was no intervention. Then, the **difference** between this value and the measured value in the **treatment** group is our estimate of the average treatment effect.

Go back to the last page and manipulate the starting values a bit more and see what impact they have on the outcome here, and then come back!

### Parallel trends

One critical assumption of the DiD approach is that the change over time between the *control* and *treatment* gropus is parallel, i.e. that the change between pre- and post would be the same for both groups in the absence of intervention. 

Now, let's take a look at what happens when we violate this assumption by increasing or decreasing the rate of change for the experimental group vs. the control group. Moving the value up will make greater change happen more quickly in the experimental vs. control groups.

```{r, echo=FALSE}
  sliderInput("beta_t_delta", "Trend difference:", min = -3, max = 3, value = 0)
```

When might this happen? Remember, we are interested in selection of high or low-risk individuals into the control and experimental groups. What if the people who made changes in their lives, e.g. moving to an environment that facilitated more opportunities for exercise, did so because they were already into fitness? Then, they might see more positive changes than the control group based on who they are. Similarly, if they were more likely to do so because of concerns about worsening health, they might already be on a downward trajectory, and we'd expect to see rises in blood pressure that might make it hard to 'see' the protective benefits of the treatment, even if they were better off than if they had not had it at all.

In sum, no framework is a substitute for judgment and for knowing your data! But the difference-in-differences approach can be very helpful for isolating a causal effect when you have reason to believe its assumptions will hold!

```{r, context="server", code=readLines("src/server.R")}

```

